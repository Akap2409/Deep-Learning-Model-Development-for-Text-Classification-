{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GlobalAveragePooling1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the datasets\n",
        "train_file_path = 'train.csv'  # Replace with your actual file path\n",
        "test_file_path = 'test.csv'    # Replace with your actual file path\n",
        "\n",
        "train_data = pd.read_csv(train_file_path)\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# Check data columns\n",
        "print(train_data.columns)\n",
        "\n",
        "# Preprocess data\n",
        "X_train = train_data['Description'].values  # Replace 'description' with the correct column name\n",
        "y_train = train_data['Title'].values        # Replace 'label' with the correct column name\n",
        "X_test = test_data['Description'].values\n",
        "y_test = test_data['Title'].values\n",
        "\n",
        "# Tokenize the descriptions\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 100\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "num_classes = train_data['Title'].nunique() # Get the number of unique classes in the 'Title' column\n",
        "label_encoder.fit(pd.concat([train_data['Title'], test_data['Title']]))  # Fit on training labels only\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Create word embeddings using a word2vec model (optional, uses pre-trained embeddings)\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.random.uniform(-1, 1, (len(word_index) + 1, embedding_dim))\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(len(word_index) + 1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=True),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')  # Use num_classes instead of a fixed number\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(X_train_padded, y_train_encoded, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(X_test_padded, y_test_encoded, verbose=0)\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Generate a classification report\n",
        "y_pred = np.argmax(model.predict(X_test_padded), axis=-1)\n",
        "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-Q8vFKB78uT",
        "outputId": "fc142ec0-3cb8-4c63-cab9-b6ea07745829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Class Index', 'Title', 'Description'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1379s\u001b[0m 367ms/step - accuracy: 1.5836e-04 - loss: 11.8830 - val_accuracy: 0.0011 - val_loss: 12.7754\n",
            "Epoch 2/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1413s\u001b[0m 370ms/step - accuracy: 8.9820e-04 - loss: 11.7671 - val_accuracy: 6.5789e-04 - val_loss: 14.2474\n",
            "Epoch 3/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1380s\u001b[0m 364ms/step - accuracy: 9.9229e-04 - loss: 11.5053 - val_accuracy: 0.0013 - val_loss: 15.5045\n",
            "Epoch 4/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1360s\u001b[0m 363ms/step - accuracy: 0.0024 - loss: 10.5495 - val_accuracy: 0.0016 - val_loss: 16.0634\n",
            "Epoch 5/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1473s\u001b[0m 393ms/step - accuracy: 0.0146 - loss: 8.4196 - val_accuracy: 0.0041 - val_loss: 17.9263\n",
            "Epoch 6/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1425s\u001b[0m 367ms/step - accuracy: 0.1645 - loss: 5.2417 - val_accuracy: 0.0074 - val_loss: 21.2474\n",
            "Epoch 7/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1361s\u001b[0m 363ms/step - accuracy: 0.4913 - loss: 2.5807 - val_accuracy: 0.0104 - val_loss: 24.9370\n",
            "Epoch 8/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1403s\u001b[0m 374ms/step - accuracy: 0.7236 - loss: 1.2988 - val_accuracy: 0.0109 - val_loss: 27.1030\n",
            "Epoch 9/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1364s\u001b[0m 364ms/step - accuracy: 0.8360 - loss: 0.7594 - val_accuracy: 0.0116 - val_loss: 29.2453\n",
            "Epoch 10/10\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1414s\u001b[0m 377ms/step - accuracy: 0.8882 - loss: 0.5148 - val_accuracy: 0.0117 - val_loss: 30.4733\n",
            "Training Accuracy: 0.9370750188827515\n",
            "Test Accuracy: 0.011710526421666145\n",
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 99ms/step\n"
          ]
        }
      ]
    }
  ]
}